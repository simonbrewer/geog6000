---
title: "GEOG 6000 Lab 06 Modeling 4"
author: "Simon Brewer"
date: "August 31, 2020"
output:
  html_document:
    toc: true
    toc_float: true
    fig_caption: true
    css: "../style.css"
header-includes:
   - \usepackage{tabularx}
---

```{r include = FALSE}

knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center")

options(width = 50)

set.seed(1234)

```

This lab will introduce you to mixed effects (hierarchical) modeling and briefly to generalized additive modeling. It requires several additional libraries to be loaded: **nlme** and **lattice**, which are both part of the standard R installation, and **lme4**, which you will need to install for this lab. 

Before starting the lab, you will need to set up a new folder for your working directory. Go to your `geog6000` folder now and create a new folder for today's class called `lab06`. The following files will be used in this lab, all available on Canvas:

- The Math achievement dataset  *MathAch.csv*
- A dataset of introduced species   *speciesIntro2.csv*
- A dataset of daily ozone concentrations   *ozone.csv*

You will need to download these files from Canvas, and move them from your `Downloads` folder to the `datafiles` folder that you made previously. 

Now start RStudio and change the working directory to `lab06`. As a reminder, you can do this by going to the [Session] menu in RStudio, then [Change working directory]. This will open a file browser that you can use to browse through your computer and find the folder. 


**With all the examples given, it is important to not just type in the code, but to try changing the parameters and re-running the functions multiple times to get an idea of their effect.** Help on the parameters can be obtained by typing `help(functionname)` or `?functionname`. 

# Hierarchical Linear Models

Hierarchical linear models allow you to extend the standard linear model to allow for variation in the model coefficients. So rather than having a single slope or intercept for the relationship between $y$ and $x_1$, we allow this to vary between different groups of observations. In this way the model tries to follow a natural hierarchy in the data, where the global model can vary according to a higher level classification (the group membership). 

## Data

For this section, we will make the model linking scores on a math achievement test to student socio-economic status (SES). Each student belongs to a school, and there are two different categories of schools. The first level model is simply the relationship between individual students math scores and SES. The second level model allows this to vary between schools. We include the school type as an additional variable in the model. 

To start, load the data file. 

```{r}

MathAch <- read.csv("../datafiles/MathAch.csv")

str(MathAch)

```

To ensure that the model is correctly made, we will need to carry out three operations. First, convert the sector variable into a factor. We tell R to use 'Public' as the first level class. All coefficients derived in the model will now be *relative* to this level (i.e. they represent the difference in Catholic schools relative to Public).

```{r}

MathAch$sector <- factor(MathAch$sector, levels=c("Public","Catholic"))

```

Now create a vector of mean SES values per school, using the `apply()` function. This iterates a function (mean) over a vector (ses), applying by group (school). This requires two steps, first we calculate the means fro each school, then we repeat each of them by the number of students in each school:

```{r}

mses <- tapply(MathAch$ses, MathAch$school, mean)

MathAch$meanses <- mses[as.character(MathAch$school)]

```

Create a vector of centered SES scores per student. To do this, simply subtract the school mean SES (calculated in the previous step) from the student raw score:

```{r}

MathAch$cses <- MathAch$ses - MathAch$meanses

```

Finally, make a simple boxplot of math score by school type, and a boxplot of SES by school type:

```{r}

plot(mathach ~ sector, data=MathAch)

plot(ses ~ sector, data=MathAch)

```

## Model

Now build a hierarchical linear model. The model design is as follows: we wish to explain the relationship between Math score and SES in the two school sectors, but we wish to account for variability between schools. The Math $\sim$ SES relationship is the first level model, and the variation among schools and sectors comprise the second level model. The full model equation is (refer to the lecture notes to see the derivation of this equation):

\begin{equation}
\begin{array}{lcl}
mathach_{ij} & = & \gamma_{00} + \gamma_{01} meanses_{j} + \gamma_{02} sector_{j} + \gamma_{10} cses_{ij} \\
& & + \gamma_{11} meanses_{j} cses_{ij} + \gamma_{12} sector_{j} cses_{ij} \\
& & + u_{0j} + u_{1j} cses_{ij} + \epsilon_{ij}
\end{array}
\end{equation}

Note that the centered SES scores have an interaction term with mean SES and sector, and we include both the slope of the centered SES/Math relationship and the intercept as random effects.

To build this model in R, load the **nlme** library, and use the `lme()` function:

```{r}

library(nlme)

math.lme1 <- lme(mathach ~ meanses * cses + sector * cses, 
                 random = ~ cses | school, 
                 data = MathAch)

summary(math.lme1)

```

The output from the `summary()` command consists of several sections:

- The first panel gives the AIC (Akaike information criterion) and BIC (Bayesian information criterion), which can be used for model selection, along with the log of the maximized restricted likelihood.
- The next panel displays estimates of the variance and covariance parameters for the random effects, in the form of standard deviations and correlations (see below for interpretation)
- The table of fixed effects is similar to output from `lm()`; to interpret the coefficients in this table, refer to the hierarchical form of the model given in the equation above.
- *Intercept*: Average math achievement score in public schools (for a perfectly average student in a perfectly average public school)
- *sectorCatholic*: Difference of average math achievement in Catholic schools. So all else being equal, students at Catholic schools do better
- *cses*: Average slope in public schools (i.e. the rate of increase in math score for a unit increase in centered SES, for students in a perfectly average public school)
- *cses:sectorCatholic*: Difference of average slope in Catholic schools. All else being equal, student math scores are less affected by their SES level in Catholic schools as the slope is lower. 
- *meanses*: Relationship of schools' average level of math achievement to their average level of SES. In other words, this tells us how the school-level characteristics are related to each other. For a one unit increase in mean school SES, the average school math score increases by about 5.3 points (note that this is the same for both sectors)
- *meanses:cses*: Within school slope change for one unit increase in mean SES. This final coefficient tells us about the impact of the mean school level SES on the relationhsip between math and SES for individuals at that school. As this is positive, this implies that as the overall mean SES of a school increases, the within school effect of SES on math score also increases. 

We can check to see how the variance in the dataset is partitioned among the levels of the model, using the `VarCorr()` function. 

```{r}

VarCorr(math.lme1)

```

The first column gives the variance explained by the random effects:

- The first line gives the amount explained by variation in intercepts among schools (6\%)
- The second line the amount due to variation in slope (0.25\%)
- The last gives the residual amount (94\%)

In general, this can be used to assess the relative contribution of each random effect. If the variance is relatively large (in general $>5\%$), then the variation of that coefficient between the different groups is important, and worth retaining in the model. 

## Testing random effects

We can now use this model to test if the inclusion of random effects makes a significant difference to the model. This can simply be done by deleting the random effects from the previous model, and using `anova()` to compare the models (using the log-likelihood score). To do this, we use a new function: `update()` which allows you to take an existing model and add or delete terms. This is often preferable to writing out a complete new model, especially when the model has a large number of terms. First remove the slope of cses as a random effect across schools (i.e. use a single slope for all schools), by updating the random effect part of the model to only include the intercept ($\sim 1$):

```{r}

math.lme2 <- update(math.lme1, random = ~ 1 | school)

anova(math.lme1, math.lme2)

```

The anova results show no significant change in the model when this effect is excluded, suggesting that the variation of slopes among schools is not a significant effect to keep in a final model. This fits what we saw in the amount of variance explained: only a tiny amount ($\sim0.25\%$) was explained setting the slope to a random effect. 

Now do the same, but exclude the random effect of intercepts between schools. As intercepts are included by default, we need to use the following specification ($-1$) to remove them:

```{r}

math.lme3 <- update(math.lme1, random = ~ cses - 1 | school)

anova(math.lme1, math.lme3)

```

This time, there is a significant change in the model, but the metrics of goodness-of-fit (AIC, BIC, log-likelihood) show that the new model is worse. In this case, including the intercept as a random effect results in a better model.

We can conclude from this that the mean SES varies across schools, but the relationship between SES and math scores shows only negligible differences.

# Generalized Hierarchical Linear Models

## Data

The file *speciesIntro2.csv* contains a simulated dataset related the success or failure of 60 species introduced into 4 different countries. The simple goal of this analysis is see if there is a difference in the overall success in different countries. As there are several different species per country, we include the intercept as a random effect across species. First load and examine the data: 

```{r}

sppint <- read.csv("../datafiles/speciesIntro2.csv")

str(sppint)

```

The table gives for each species and location the number of successful and failed introductions. As we want to model the overall success, we need to first convert this to proportion, which we can then model as a binomial process. We first calculate the sum of *trials* (the number of each species that were introduced), then divide the number of successes by this to get the proportion. 

```{r}
sppint$total = sppint$success + sppint$failure
sppint$prop = sppint$success / sppint$total
```

And visualize the success rate as a proportion of trials:

```{r}
boxplot(prop ~ location, sppint)
```

## Model

As the success variable is proportional ([0,1]), we use the `glmer()` function from the **lme4**. This uses the same arguments as the `glm()` models we looked at earlier this semester, with a `family` parameter to specify the distribution of the dependent variable. As this is a binary variable, we specify it as a two-column matrix (success and failure) in the model, using `cbind()`. As we are modeling proportions, we also need to include the `weights` argument to specify the total number of trials for each species. 

Note that the syntax here is a little different from the previous example; random effects are not specified as a separate parameter, but are included directly in the model formula. These are specified as an effect/variable pair; in the example below `(1|species)` means vary intercept across species groups.

```{r message = FALSE}

library(lme4)

spp.glmm.1 <- glmer(prop ~ location + (1|species), 
                    family = binomial, weights = total,
                    data = sppint)

summary(spp.glmm.1)

```

The results tell us that the log-odds of successful introduction in Argentina (the reference location) are -0.615, or odds of about 0.54. The coefficients for the different countries give an estimate of the change in log-odds, with introductions in Australia being lower than Argentina, but introductions elsewhere being more successful (although there is little significance in the results). 

## Model comparisons

Now test if location has an impact on species success or failure, by creating a new model without location, and using anova to compare models. Note that we could also use the `update()} function to achieve this.

```{r}

spp.glmm.2 <- glmer(prop ~ 1 + (1|species), 
                    family = binomial, weights = total,
                    data = sppint)

#spp.glmm.2 = update(fit.glmm.1, .~. -location)

anova(spp.glmm.1, spp.glmm.2)

```

The results of the anova show a slight, but significant improvement in the first model (lower AIC, log-likelihood). In this cases, we would retain location as significant information in the model.

# Generalized Additive Models

```{r}

ozone <- read.csv('../datafiles/ozone.csv')

```

Generalized Additive Models (GAMs) use methods to fit the model to the data locally, using splines to account for non-linear relationships. They also include the possibility to specify the distribution family and link function for the dependent variable. Here we will fit a GAM to a dataset of ozone concentrations covering several months.

- Load the ozone data into a data frame called 'ozone'. 
- Use the `pairs()` function to make scatterplots between the variables. 
- Are there any clear relationships with the ozone concentrations?

We will now build a GAM linking ozone to air temperature and wind speed. GAMs are fit and explored using an add-on package called **mgcv**. Similarly to the **splines** packages, this is installed by default with a standard R installation, but is not loaded when R is first started. Load it now using the `library()` function:

```{r message = FALSE}

library(mgcv)

```

To test that it has loaded, open the help page for the `gam()` function:

```{r eval=FALSE}

help(gam)

```

The `gam()` function uses the usual formula syntax in R ($\sim$). However, the independent variables can either be specifed with a smoothing function for local fitting, or without, in which case a generalized linear fit will be obtained. The smoothing functions include `s()` for smoothing splines and `te()` for a tensor product to describe the interaction between two independent variables. By default, `gam()` chooses the number of knots in the spline using generalized cross-validation, but if needed, you can manually set knots using the `knot` parameter.

We'll build a GAM model for the ozone data, using temperature and wind speed as predictors:

```{r}

ozone.gam <- gam(ozone ~ s(temp) + s(wind), data = ozone)

summary(ozone.gam)

```

Note that as we are using a normal (gaussian) family for the dependent variable, we do not need to specify the family or link function, which by default are set to 'gaussian' and 'identity' respectively. If we were using the `gam()` function with binomial or count data, we can set these using the same syntax as we used with GLMs. 

- In the output of the `summary()` function, look for the significance tests on the smoothed independent variables, as well as the $r^2$ and the deviance explained. 
- Does this appear to a good job of modeling the ozone variations?

Now plot the model obtained for temperature and ozone:

```{r}

plot(ozone.gam, 
     resid = TRUE, 
     pch = 16, 
     select = 1)

```

- Plot the relationship with wind speed, by changing the parameter `select` to 2.

As an alternative, we can use a perspective plot to look at the relationship of ozone to both temperature and wind speed. The viewing direction is controlled by the parameters `theta` (horizontal) and `phi` (vertical).

```{r}

vis.gam(ozone.gam, 
        theta = 230, 
        phi = 20)

```

- Try rotating the plot to get the best view of the surface.

As the observations are from successive days over a four month period, we can try plotting the original and fitted model as a time series. We will simple make an time index vector to provide the 'x'-variable, plot the original ozone values, then overlay the modeled values:

```{r}

timeIDX <- seq(1, length(ozone$ozone))

plot(timeIDX, 
     ozone$ozone, 
     type = "l", 
     xlab = "Time", 
     ylab = "Ozone Conc ppb")

lines(timeIDX, fitted(ozone.gam), col = 2)

legend("topleft", 
       legend = c("Obs.", "Est."),
       lty = 1, 
       col = c(1,2))

```

Finally, use the model to predict the ozone concentration for a warm, still day:

```{r}

newclim <- data.frame(temp = 85, wind = 5)

predict(ozone.gam, newdata = newclim, se.fit = TRUE)

```

- Redo the prediction with an increase of wind speed to 15 m/s

\newpage

# Where to get help

```{r, child = '../get-help.Rmd'}
```

# Files used in lab

## Math achievement dataset: *MathAch.csv*
| Column header | Variable |
| --- | --- |
| school | School ID |
| ses | Student socio-economic status |
| mathach | Student math score |
| sector | School type (Catholic/Public) |

## Species introduction dataset: *speciesIntro.csv*
| Column header | Variable |
| --- | --- |
| location | Country of introduction |
| species | Species |
| sample.size | Sample size |
| success | Successful introduction (0/1) |
| failure | Failed introduction (1-success) |

## Daily ozone concentration data set: *ozone.csv*
| Column header | Variable |
| --- | --- |
| rad | Solar radiation (Ly) |
| temp | Air temperature (F) | 
| wind | Wind speed (m.s-1) | 
| ozone | Ozone concentrations (ppb) |

