---
title: "GEOG 6000 02 Inference Lab"
author: | 
  | Simon Brewer
  | Geography Department
  | University of Utah
date: "August 19, 2020"
output:
  html_document:
    toc: true
    toc_float: true
    fig_caption: true
    css: "../style.css"
header-includes:
   - \usepackage{tabularx}
---

In this lab, we will look at using R for statistical inference tests. Before starting the lab, you will need to set up a new folder for your working directory. In the previous lab, you should have set up a `geog6000` folder, containing a folder for the first lab (`lab01`) and a folder for the data files (`datafiles`). Go to this folder now and create a new folder for today's class called `lab02`. The folder structure should now look like this:

For this class, the labs will assume that you have your files organized according to the following structure:

```
+-- geog6000
|   +-- datafiles
|   +-- lab01
|   +-- lab02
```

We will be using the following files for these examples:

- A dataset from a study of Irish schoolchildren *irished.csv*
- Body temperature dataset: *normtemp.csv*
- Soil porosity dataset: *soil-porosity.txt*
- Land-cover and elevation dataset: *landcover.csv*
- Measurements of dissolved oxygen from locations around an estuary: *maximumDO.txt*

You will need to download these files from Canvas, and move them from your `Downloads` folder to the `datafiles` folder. 

Now start RStudio and change the working directory to `lab02`. As a reminder, you can do this by going to the [Session] menu in RStudio, then [Change working directory]. This will open a file browser that you can use to browse through your computer and find the folder. 

**With all the examples given, it is important to not just type in the code, but to try changing the parameters and re-running the functions multiple times to get an idea of their effect.** Help on the parameters can be obtained by typing `help(functionname)` or `?functionname`. 

# Working with data frames in R

## Reading and writing files

Larger data sets will most commonly be read in from files. A recommended format to use are csv (comma-separated value) files, as these may be easily exchanged with spreadsheet software. Use the `read.csv()` function to read in the data from the Irish Education file:
(*irished.csv*).

```{r results='hide'}
irished <- read.csv("../datafiles/irished.csv")
```

Plain text (or tab delimited files) can be read with the function `read.table()`, and an example is given later in this lab. Note that we use a variable to store the values read in from this file. By default, R creates this as a data frame, as this allows different types of variables to be stored (you can check this with the `class()` function). To see what is in the new data frame you can examine the *structure* as follows:

```{r}
str(irished)
```

Which tells you what the individual columns of the data frame are called (e.g. `sex`, `DVRT`), and the type of data (here all integer data). Note there are several add-on functions that will allow you to read from files with different formats, including most other statistical software.

## Review of indexing

The Irish Education dataset that you read in in the previous section has 500 rows (observations) and 6 columns (variables). R has two methods of indexing or selecting subsets of the data. The first of these uses a system of coordinates within the grid of data, so `irished[4,1]` will select the value in the fourth row of the first column. Leaving one of these indices blank will select all values, so `irished[4,]` will select all values in the fourth row. A range of values can be selected as follows:

```{r results='hide'}
irished[1:10,4:6]
```

This returns the values in from columns 4 thru 6 in lines 1 thru 10. 

The second form of indexing uses the column names directly, with an intervening `$`. This can be useful with large datasets, when the number of variables makes it hard to locate the value of interest, or if the order of the variables may change in the file. Using the column names also makes it easier to remember what you are doing. To extract the DVRT score (a vocal reasoning score):

```{r results='hide'}
irished$DVRT
irished$DVRT[1:10]
```

Note that we can combine the two approaches to select only the first ten values from the DVRT column.

We can equally use this notation to create new variables in the data frame. For example, if we want to make a new variable as the log-transformed DVRT score, we can do the following:

```{r}
logDVRT <- log(irished$DVRT)
```

More usefully, we can store this in the existing data frame as follows:

```{r}
irished$logDVRT <- log(irished$DVRT)
```

We can now write the data back out to a file to save the new data frame:

```{r}
write.csv(irished, "irished2.csv")
```

This will write a new csv file called *irished2.csv* in your working directory (note the `datafiled` folder), containing the `irished` data frame, with the new column. Note that R uses the variable names as column headers and adds a column with line numbers. You can remove the line numbers by adding the parameter `row.names=FALSE` to the `write.csv()` function. 

## Working with factors

The dataset contains some categorical variables (e.g. presence/absence of leaving certificate, sex). As these are coded as 0/1 in the file, R has read them as numerical value. Before moving on, we will convert these to *factors*, which R will understand as categorical data. To do this we use the function `factor()`, and create new variables in the data frame:

```{r results='hide'}
irished$fsex <- factor(irished$sex, 
                       levels = c(1, 2),
                       labels = c("male", "female"))

irished$flvcert <- factor(irished$lvcert, 
                          levels = c(0, 1), 
                          labels = c("not taken", "taken"))
```

The function `factor` has an additional parameter `ordered`. This can be used to define factors where the `order` is important (e.g. small, medium, large), as opposed to unordered groups (e.g. makes of cars). By default, this is set to unordered. 

## Indexing by value

A more useful approach to indexing is to select observations according to some criteria, for example all observations over 50, or all female students, allowing the creation of subsets for comparative studies. There are a couple of different ways to do this, but one of the most straightforward uses the `which()` function. This uses a conditional statement to return a list indexes for values of a variable that correspond to a criteria. So if we wanted to obtain the index of all students with a DVRT score over 100:

```{r results='hide'}
which(irished$DVRT > 100)
dvrt100 <- which(irished$DVRT > 100)
```

By storing this set of indices as new variable, we can then use it to select the data subset for further analysis.

```{r results='hide'}
irished$DVRT[dvrt100]
irished$sex[dvrt100]
```

We can now use this index, for example, to compare the number of students taking the leaving certificate from the group with a DVRT score greater than or less than 100.

```{r results='hide'}
table(irished$lvcert[dvrt100])
table(irished$lvcert[-dvrt100])
```

Note that using the '-' in front of the index means 'select all values not in the index set'.

## Implicit loops

The `tapply()` function is also useful for exploring datasets. If, for example, we were interested in calculating the mean DVRT score for boys and girls, we can run the following line:

```{r}
tapply(irished$DVRT, irished$fsex, mean)
```

Note that this works for more than two groups. To get the mean DVRT score for the different school types (`schltype`)

```{r}
tapply(irished$DVRT, irished$schltype, mean)
```


# Inference tests

## Tests for comparing means and variance

### Two sample *t*-test

There are several different forms of the $t$-test. The two-sample test compares the means of two vectors of observations (this is the example we went through in class). For this we will use a small set of observations recorded from soil samples in two different locations (`A` and `B`). The porosity of the soil (the amount of air in a unit measure of soil) was recorded for each observation, which gives us an estimate of compaction. We are interested in knowing whether these values are the same or different at the two locations. The data are in the file *soil-porosity.txt*. As this is a plain text file, not a csv-file, we read it in using the `read.table()` function. This file does not have a header, so we use the parameter `col.names` to give each column a name:

```{r results='hide'}
poro <- read.table("../datafiles/soil-porosity.txt", col.names = c("A","B"))
```

Start by running the `summary()` function on the new data frame (`poro`) to look at the mean values. How big is the difference in the means? 

Now write out the null and alternative hypotheses for this dataset and this question. Be specific, i.e. use the variable names. What is the null ($H_0$)? What is the alternative ($H_a$)?

Before running the $t$-test, open the help page for the R function (`help(t.test)`). This gives you the full set of parameters or arguments that can be used. Note that that the first two are two vectors (`x` and `y`), so we will use the two vectors from our data frame. The argument called `alternative`, states whether we are running a one- or two-sided test. Here, we are simply going to run a two-sided test to look for any difference (positive or negative). 

```{r}
t.test(poro$A, poro$B, alternative = 'two.sided')
```

This gives a $p$-value of approximately 0.0059, so (assuming a critical level of 0.05) we can reject our null hypothesis with a reasonable amount of confidence (this suggests we have about a 1-in-170 chance of making a mistake in our conclusion). Try changing the alternative to `greater` or `less` and look at how the $p$-value changes when you specify a one-sided test, and how this relates to the two means. 

### One sample *t*-test

For the second example, we will run a one-sample $t$-test. As the name implies, this only uses one sample or set of observations, and compares them to a single, usually theoretical value. This is useful if you want to compare your observations to a value that has been established from previous studies or from first principles. Here, we'll test a set of adult body temperatures, with the goal of testing if the assumed average body temperature of 98.6F is supported by temperatures taken from 130 adults, or if there is a significant difference. 

We begin by loading the data file containing the body temperature results:

```{r}
normtemp = read.csv("../datafiles/normtemp.csv")
names(normtemp)
```

- Calculate the mean and standard deviation of the temperature data (column `temp`).  Does this suggest that the sample temperatures are different from the hypothetical value? As a reminder: to use a single column from a dataframe, use either the `$` notation (e.g. `normtemp$temp` or the [row,column] indexing  (e.g. `normtemp[,1]`). 

Now we first form our null and alternative hypothesis. We will use a two-sided *t*-test for differences.

- Null $H_0$: Mean temperature of samples $= 98.6$
- Alternative $H_a$: Mean temperature of samples $\ne 98.6$

Now we run the test, using the `t.test()` function. Rather than using two vectors, as in the previous example, we supply the vector of values (the sample) the argument `mu`, which defines the assumed or hypothetical value we wish to test against. We also set the parameter `alternative`, which states whether this is two-sided or one-sided:

```{r}
t.test(normtemp$temp, mu = 98.6, alternative = "two.sided")
```

This gives a $p$-value of approximately 2.4e-7, so we can reject our null hypothesis with high confidence (this suggests we have about a 1-in-4 million chance of making a mistake in our conclusion). Try changing the alternative to `greater` or `less` and look at how the $p$-value changes when you specify a one-sided test. 

- Would you keep or reject the null hypothesis with these results?

### $F$-test

The $F$-test tests for equality of variance between two samples. As such, it is usually carried out prior to other tests to check assumptions (the basic $t$-test assumes equal variance). It is calculated as the ratio of the two variances ($s^2_a / s^2_b$). If the variances are similar, $F$ will be close to 1. If these differ, then $F$ will increase (or tend to zero, depending on how the ratio is calculated). We then compare the observed $F$-statistic to an $F$-distribution, to see the chance that our value arose through simple sampling effects. The $F$-distribution is defined by two degrees of freedom ($n-1$ for each of the two samples). We will test use this to test the two soil porosity samples used in the previous section. 

- As before, write out your hypotheses prior to running the code.  

The function `var.test()` carries out this test in R:

```{r results='hide'}
var.test(poro$A, poro$B)
```

- What evidence does this provide for differences in variance between the two samples?

As there is little support for a difference in variances, we can re-run the $t$-test from the first example. By default, this method assumes that the variances are unequal, and adjusts the test results accordingly. As the $F$-test suggests that the variances are equal, we can re-run our test, setting the argument `var.equal` to true:

```{r results='hide'}
t.test(poro$A, poro$B, var.equal = TRUE)
```

How does this compare to the previous $t$-test? What has changed in the results (and how has it changed)? Does this change your conclusions?

## Tests of association

### Correlation tests

The correlation coefficient obtained between two sets of variables may be tested for significant differences from a null correlation. The parametric correlation coefficient (Pearson's $r$) can be used to calculate a $t$-statistic, and then compared to a $t$-distribution. For data that is not normally distributed, Spearman's rank correlation can be used. For each variable, all values are converted to ranks (the lowest value is converted to 1, the second lowest to 2, ..., the highest to $n$). We then calculate Pearson correlation coefficient based on the correlation between these ranks for the two variables and use this to generate a $t$-statistic. 

Here we generate two random sets of 30 numbers, and test them using Pearson's correlation coefficient. Our null hypothesis ($H_0$) is that they do not correlate, which is highly likely, given that they are randomly generated.

- Given this, write out the alternate hypothesis.

```{r results='hide'}
x <- rnorm(30, 10, 1)
y <- rnorm(30, 20, 2)
cor.test(x, y, method = "pearson")
```

- Do the results support the null or alternate hypothesis?

In a second test, we do the same, but we replace the second series with a duplicate of the first, with some added noise. Note the second use of the `rnorm()` function with a much smaller mean and s.d. to add noise to a set of data (see also `jitter()`). 

```{r results='hide'}
x <- rnorm(30, 10, 1)
y <- x + rnorm(30, 1, 0.5)
cor.test(x,y, method = "pearson")
```

- Do the results support correlation between x and y? Why do you get these results?

### Contingency tests

These test for *association* between two sets of categories. The null hypothesis is that values are uniformally distributed among categories. If association exists, we expect that some pairwise associations between categories will be high and other low. 

In this example, we have observations of land cover classes from a set of 80 locations, which are also classified as low or high elevation. Our question is then if there is association between land cover class and elevation class, i.e. are some land cover classes more commonly found in some elevation classes, or are they even spread among the different elevations? 

- What do you think the null and alternative hypotheses would be for this test?

We start by reading the data in from a csv file (*landcover.csv*), then creating the contingency table showing association. For this we use the function `table()`, which cross-tabulates pairs of categories. 

```{r}
lc <- read.csv("../datafiles/landcover.csv")
lc.tab <- table(lc$elevation, lc$landcover)
print(lc.tab)
```

With this done, we can now use the Chi-squared test to test the significance of the association. This test compares the observed number in each cell of this table, to the expected number if the observations were even spread out. The expected value is for a cell on row $i$ and column $j$ is calculated by multiplying the total number of observations in that row by the total number in that column, and dividing by $n$, the total number of observations. The test then calculates the Chi-squared statistic as:

\[
\chi^2=\sum^n_{i=1} \frac{(O_{ij}-E_{ij})^2}{E_{ij}}
\]

The resulting test statistic is then compared against a Chi-squared distribution, where the shape is defined by the degrees of freedom based on the number of groups involved

- As before, note your hypotheses before running this code. 

```{r results='hide'}
chisq.test(lc.tab)
```

- Given your hypotheses, do the results support associations between land cover and elevation? How strong is the significance of your results?

# Analysis of variance

Analysis of variance (ANOVA) has several uses in statistical analysis, but here we will use it as an extension of a $t$-test to look for differences between more than two groups. The name comes from the use in analyzing or decomposing the variance in a sample into two parts: the variance explained by (or *between*) the groups and the residual (or *within*) group variance. 

## One-way ANOVA

We will use one-way ANOVA to examine differences in dissolved oxygen (DO) measurements from a set of four locations in the file *maximumDO.txt*. Download and read this file into R:

```{r results='hide'}
maxdo <- read.table("../datafiles/maximumDO.txt", header = TRUE)
maxdo
class(maxdo)
```

The data is presented as a data frame of values, one column per location. In order to use this in an ANOVA, we need to create two vectors: one with all the DO values, and one with a code for each location. 

- For the first of these, we use the `c()` function to concatenate the individual observations into a single vector
- The vector of location codes is created by repeating the letters 'A' though 'D' 10 times (the number of observations)
- Finally, we put the new vectors into a new data frame, to keep the data together

```{r}
dovals <- c(maxdo$A, maxdo$B, maxdo$C, maxdo$D)
locs <- rep(LETTERS[1:4], each = 10)

maxdo <- data.frame(location = locs, maxdo = dovals)
```

Use this data frame to make a boxplot of the values across the four locations. Note we use the formula syntax to indicate which variable is the response variable (DO), and which is the independent or explanatory variable (location). As a reminder, the formula syntax is always written as `response ~ explantory`, and we will be using this extensively in building statistical models. 

```{r fig.keep='none'}
boxplot(maxdo ~ location, 
        data = maxdo,
        xlab = "Location", 
        ylab = "mg/l")
```

- Prior to running the ANOVA function, state the null and alternative hypotheses

We now run the ANOVA, using the R function `aov()`. This uses the same formula syntax, and produces the breakdown of variance as the sum of squares. The first column gives the amount explained by the location and the remaining variance is given in column 2. 

```{r}
aov(maxdo ~ location, data = maxdo)
```

You should be able to see from this that the majority of variance is explained by our grouping. To test the significance, we could calculate the $F$-statistic and compare to a $F$-distribution. However, if we use the `summary()` function, R will do this for us:

```{r results='hide'}
summary(aov(maxdo ~ location, data = maxdo))
```

Now we obtain the sum of squares for each component, the mean (sum of squares divided by d.f.), the $F$-statistic and the $p$-value for the test. 

- Do the results support a difference in DO values between locations?


# Exercises

1. The dataset on body temperatures includes a column coding male (1) and female (2) subjects. Use a $t$-test to look for differences in temperature between males and females. *Hint: you can do this by either splitting the temperature values into two new vectors or you can use the formula syntax (see the ANOVA example)* 
    + State the null and alternate hypotheses
    + Carry out the $t$-test and report the $t$-statistic and the $p$-value obtained
    + On the basis of this state whether or not you have evidence for a difference in body temperature between men and women 

2. The file *gapC.csv* contains socio-economic information for 173 countries from the GapMinder dataset. Each country has been assigned to one of seven geographical regions, roughly corresponding to the continents. Carry out a one-way analysis of variance of the life expectancy variable to look for differences across the different regions. 
    + State the null and alternate hypotheses
    + Make a boxplot of life expectancy per continent
    + Carry out the ANOVA and give the $F$-statistic and the $p$-value obtained
    + On the basis of this state whether or not life expectancy varies across continents



<!-- ## Implicit loops -->
<!-- Most R functions will treat an entire matrix or dataset as a set of values. There are some useful functions that allow these to be treated in more useful way, by *applying* a function to various subsets of the data using implicit loops. -->

<!-- Read in the Iris data set (*iris.csv*). -->
<!-- ```{r results='hide'} -->
<!-- iris <- read.csv("iris.csv") -->
<!-- ``` -->
<!-- The `apply()` function allows you calculate means, sums, etc for rows or columns of a matrix or data frame. The first parameter is the data object and the second parameter gives the dimension over which the function (the third parameter) is to be applied. The dimensions use numbers, where 1 = rows, 2 = cols (note that you can use higher numbers with multiple dimension arrays).  -->

<!-- ```{r results='hide'} -->
<!-- apply(iris[,1:4],1,mean) -->
<!-- apply(iris[,1:4],2,mean) -->
<!-- ``` -->

<!-- The first of these calculates the mean of each row (1), the second the mean of each column (2). -->

<!-- The `tapply()` function allows you to apply a function, but using a column in the data set to define subgroups. For example, to calculate the standard deviation for the Iris sepal lengths for each of the three species: -->
<!-- ```{r results='hide'} -->
<!-- tapply(iris$Sepal.Length, iris$Species, sd) -->
<!-- ``` -->

<!-- - Try to do the same, but to calculate the mean DVRT score by sex from the Irish Ed dataset. -->

# Where to get help

```{r, child = '../get-help.Rmd'}
```

# Files used in lab

## Irish education dataset: *irished.csv*
| Column header | Variable |
| --- | --- |
| sex | Sex of student |
| DVRT | Vocal reasoning test score |
| edlevel | Mothers education level |
| lvcert | Leaving certificate taken (yes/no) |
| fathocc | Fathers occupation |
| schltype | School type |

## Body temperature dataset: *normtemp.csv*
| Column header | Variable |
| --- | --- |
| Blank | Observation number |
| temp | Body temperature (F) |
| sex | Sex |
| weight | Weight (kg) |

## Soil porosity dataset: *soil-porosity.txt* 
| Column header | Variable |
| --- | --- |
| Blank | Porosity data from location A |
| Blank | Porosity data from location B |

## Land-cover and elevation dataset: *landcover.csv*
| Column header | Variable |
| --- | --- |
| elevation | Site elevation |
| landcover | Landcover type |

## Dissolved oxygen dataset: *maximumDO.txt*
| Column header | Variable |
| --- | --- |
| A | DO values for site A |
| B | DO values for site B |
| C | DO values for site C |
| D | DO values for site D |

## GapMinder Dataset: *gapC.csv*
| Column header | Variable |
| --- | --- |
| country | Country |
| lifeexpectancy | Life expectancy |
| continent | Continent/Region |


